# -*- coding: utf-8 -*-
"""Submission Kedua Bayun.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfCzV0Mhv8fu6fWwdlURr4J3IPNaUxCO

# **Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech**

*   Nama        : Bayun Kurniawan
*   Email       : bayunk59@gmail.com
*   Id Dicoding : Bayun Kurniawan

# **Persiapan**

## Menyiapkan library yang dibutuhkan
"""

!pip install pandas sqlalchemy
from sqlalchemy import create_engine
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix
tree_model = DecisionTreeClassifier(random_state=123)

"""## Menyiapkan data yang akan digunakan"""

# Loading data

jaya_df = pd.read_csv(
    'https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/students_performance/data.csv',
    delimiter=";")
jaya_df.head(5)

# Cek missing value
jaya_df.isna().sum()

"""Tidak terdapat missing value"""

# Memeriksa tipe data tiap fitur
jaya_df.info()

jaya_df.describe()

"""# **Data Understanding**"""

# Melihat distribusi data pada fitur 'Status'

sns.countplot(x='Status', data=jaya_df)
plt.title("Distribusi Status Mahasiswa")
plt.show()

status_counts = jaya_df['Status'].value_counts()
status_percent = jaya_df['Status'].value_counts(normalize=True) * 100
status_df = pd.DataFrame({'Jumlah': status_counts, 'Persentase (%)': status_percent})
print(status_df)

"""Berdasarkan distribusi di atas, jumlah mahasiswa yang 'Dropout' lumayan banyak, berjumlah 1421 data dengan presentase mencapai 32%"""

# Distribusi fitur numerik
num_features = jaya_df.select_dtypes(include=[np.number])
# Calculate the number of rows and columns for the subplots
num_cols = 3  # Number of columns in the grid
num_rows = int(np.ceil(len(num_features.columns) / num_cols))  # Calculate rows needed

plt.figure(figsize=(15, 30))
for i, column in enumerate(num_features.columns, 1):
    plt.subplot(num_rows, num_cols, i)  # Use calculated rows and columns
    sns.histplot(jaya_df[column], bins=30, kde=True, color='blue')
    plt.title(f'Distribusi {column}')
plt.tight_layout()
plt.show()

# Melihat nilai korelasi fitur numerik dengan Status

# Pisahkan kolom numerik saja (tanpa mengubah Status)
numeric_cols = jaya_df.select_dtypes(include=['number'])

# Hitung korelasi antar kolom numerik
correlation_matrix = numeric_cols.corr()

# Cek apakah kolom 'Status' ada di original dataframe dan numerik
if 'Status' in jaya_df.columns and pd.api.types.is_numeric_dtype(jaya_df['Status']):
    Status_corr = correlation_matrix['Status'].drop('Status')
else:
    # Hitung korelasi manual antara Status (non-numerik) dan kolom numerik
    Status_corr = jaya_df[numeric_cols.columns].apply(lambda x: jaya_df['Status'].astype('category').cat.codes.corr(x))

# Urutkan korelasi berdasarkan kekuatan absolut
correlation_with_Status_sorted = Status_corr.reindex(
    Status_corr.abs().sort_values(ascending=False).index
)

# Tampilkan hasil
print("Korelasi terhadap Status (dari paling signifikan):")
print(correlation_with_Status_sorted)

"""Dapat dilihat ada beberapa fitur yang memiliki korelasi tinggi dengan Status, baik secara positif maupun negatif

# **Data Preparation / Preprocessing**
"""

# Buat salinan data
main_df = jaya_df.copy()

# Hapus data yang tidak dipakai

main_df.drop(columns=[
    "Fathers_qualification",
    "Fathers_occupation",
    "Mothers_occupation",
    "International",
    "Educational_special_needs",
    "Nacionality",
    "GDP",
    "Inflation_rate",
    "Course",
    "Mothers_qualification",
    "Unemployment_rate",
    "Curricular_units_1st_sem_credited",
    "Previous_qualification",
    "Curricular_units_2nd_sem_credited",
    "Curricular_units_1st_sem_without_evaluations",
    "Daytime_evening_attendance",
    "Application_order",
    "Marital_status",
    "Application_mode",
    "Admission_grade",
    "Curricular_units_2nd_sem_without_evaluations",
    "Displaced",
    "Previous_qualification_grade"
], inplace=True)

main_df.info()

"""Tersisa 14 fitur

## Train-test Split
"""

train_df, test_df = train_test_split(main_df, test_size=0.05, random_state=42, shuffle=True)
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

print(train_df.shape)
print(test_df.shape)

"""## Oversampling"""

sns.countplot(data=train_df, x="Status")
plt.show()

train_df.Status.value_counts()

df_majority_1 = train_df[(train_df.Status == "Graduate")]
df_majority_2 = train_df[(train_df.Status == "Dropout")]
df_minority = train_df[(train_df.Status == "Enrolled")]

from sklearn.utils import resample
from sklearn.utils import shuffle

df_majority_2_undersampled = resample(df_majority_2, n_samples=2101, random_state=42)
df_minority_undersampled = resample(df_minority, n_samples=2101, random_state=42)

oversampled_train_df = pd.concat([df_majority_1, df_majority_2_undersampled]).reset_index(drop=True)
oversampled_train_df = pd.concat([oversampled_train_df, df_minority_undersampled]).reset_index(drop=True)
oversampled_train_df = shuffle(oversampled_train_df, random_state=42)
oversampled_train_df.reset_index(drop=True, inplace=True)

sns.countplot(data=oversampled_train_df, x="Status")
plt.show()

"""## Encoding dan Scaling"""

# Encoding data kategorik
# Memisahkan feature training (X) dan target (Y)

X_train = oversampled_train_df.drop(columns="Status", axis=1)
y_train = oversampled_train_df["Status"]

X_test = test_df.drop(columns="Status", axis=1)
y_test = test_df["Status"]

# helper function

def scaling(features, df, df_test=None):
    if df_test is not None:
        df = df.copy()
        df_test = df_test.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "model/scaler_{}.joblib".format(feature))

            X_test = np.asanyarray(df_test[feature])
            X_test = X_test.reshape(-1,1)
            df_test["{}".format(feature)] = scaler.transform(X_test)
        return df, df_test
    else:
        df = df.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "model/scaler_{}.joblib".format(feature))
        return df

def encoding(features, df, df_test=None):
    if df_test is not None:
        df = df.copy()
        df_test = df_test.copy()
        for feature in features:
            encoder = LabelEncoder()
            encoder.fit(df[feature])
            df["{}".format(feature)] = encoder.transform(df[feature])
            joblib.dump(encoder, "model/encoder_{}.joblib".format(feature))

            df_test["{}".format(feature)] = encoder.transform(df_test[feature])
        return df, df_test
    else:
        df = df.copy()
        for feature in features:
            encoder = LabelEncoder()
            encoder.fit(df[feature])
            df["{}".format(feature)] = encoder.transform(df[feature])
            joblib.dump(encoder, "model/encoder_{}.joblib".format(feature))
        return df

# Buat direktori baru

if not os.path.exists('model'):
    os.makedirs('model')

numerical_columns = [
    "Debtor", "Tuition_fees_up_to_date", "Gender",
    "Scholarship_holder", "Age_at_enrollment",
    "Curricular_units_1st_sem_enrolled", "Curricular_units_1st_sem_evaluations",
    "Curricular_units_1st_sem_approved", "Curricular_units_1st_sem_grade",
    "Curricular_units_2nd_sem_enrolled", "Curricular_units_2nd_sem_evaluations",
    "Curricular_units_2nd_sem_approved",
    "Curricular_units_2nd_sem_grade"
]

new_train_df, new_test_df = scaling(numerical_columns, X_train, X_test)

# Encoder target

encoder = LabelEncoder()
encoder.fit(y_train)
new_y_train = encoder.transform(y_train)
joblib.dump(encoder, "model/encoder_target.joblib")

new_y_test = encoder.transform(y_test)

"""## Principal Component Analysis (PCA)"""

# Hitung korelasi antar kolom numerik
correlation_matrix = new_train_df.corr()

# Tampilkan heatmap korelasi
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title("Korelasi Antar Fitur Numerik (setelah Scaling)")
plt.show()

pca_numerical_columns_1 = [
    "Curricular_units_1st_sem_enrolled",
    "Curricular_units_1st_sem_evaluations",
    "Curricular_units_1st_sem_approved",
    "Curricular_units_1st_sem_grade",
    "Curricular_units_2nd_sem_enrolled",
    "Curricular_units_2nd_sem_evaluations",
    "Curricular_units_2nd_sem_approved",
    "Curricular_units_2nd_sem_grade"
]

pca_numerical_columns_2 = [
     "Debtor",
     "Tuition_fees_up_to_date"
]

train_pca_df = new_train_df.copy().reset_index(drop=True)
test_pca_df = new_test_df.copy().reset_index(drop=True)

pca = PCA(n_components=len(pca_numerical_columns_1), random_state=123)
pca.fit(train_pca_df[pca_numerical_columns_1])
princ_comp = pca.transform(train_pca_df[pca_numerical_columns_1])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical_columns_1)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical_columns_1)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

pca_1 = PCA(n_components=2, random_state=123)
pca_1.fit(train_pca_df[pca_numerical_columns_1])
joblib.dump(pca_1, "model/pca_{}.joblib".format(1))
princ_comp_1 = pca_1.transform(train_pca_df[pca_numerical_columns_1])
train_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(princ_comp_1, columns=["pc1_1", "pc1_2"])
train_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)
train_pca_df.head()

pca = PCA(n_components=len(pca_numerical_columns_2), random_state=123)
pca.fit(train_pca_df[pca_numerical_columns_2])
princ_comp = pca.transform(train_pca_df[pca_numerical_columns_2])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical_columns_2)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical_columns_2)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

pca_2 = PCA(n_components=2, random_state=123)
pca_2.fit(train_pca_df[pca_numerical_columns_2])
joblib.dump(pca_2, "model/pca_{}.joblib".format(2))
princ_comp_2 = pca_2.transform(train_pca_df[pca_numerical_columns_2])
train_pca_df[["pc2_1", "pc2_2"]] = pd.DataFrame(princ_comp_2, columns=["pc2_1", "pc2_2"])
train_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
train_pca_df.head()

test_princ_comp_1 = pca_1.transform(test_pca_df[pca_numerical_columns_1])
test_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(test_princ_comp_1, columns=["pc1_1", "pc1_2"])
test_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)

test_princ_comp_1 = pca_2.transform(test_pca_df[pca_numerical_columns_2])
test_pca_df[["pc2_1", "pc2_2"]] = pd.DataFrame(test_princ_comp_1, columns=["pc2_1", "pc2_2"])
test_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
test_pca_df.head()

"""# **Modelling**"""

# Grid Search CV

param_grid = {
    "penalty": ["l1","l2"],
    "C": [0.01, 0.1, 1]
}

log_model = LogisticRegression(random_state=123)

CV_lr = GridSearchCV(estimator=log_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_lr.fit(train_pca_df, new_y_train)

"""## Decision Tree"""

# Decision Tree

tree_model = DecisionTreeClassifier(random_state=123)

param_grid = {
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5, 6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_tree = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_tree.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_tree.best_params_)

tree_model = DecisionTreeClassifier(
    random_state=123,
    criterion='entropy',
    max_depth=8,
    max_features='sqrt'
)

tree_model.fit(train_pca_df, new_y_train)
joblib.dump(tree_model, "model/tree_model.joblib")

"""## Random Forest"""

# Random Forest

rdf_model = RandomForestClassifier(random_state=123)

param_grid = {
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_rdf = GridSearchCV(estimator=rdf_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_rdf.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_rdf.best_params_)

rdf_model = RandomForestClassifier(
    random_state=123,
    max_depth=8,
    n_estimators=500,
    max_features='sqrt',
    criterion='gini',
    n_jobs=-1
)
rdf_model.fit(train_pca_df, new_y_train)
joblib.dump(rdf_model, "model/rdf_model.joblib")

# Gradien Boosting
gboost_model = GradientBoostingClassifier(random_state=123)

param_grid = {
    'max_depth': [5, 8],
    'n_estimators': [200, 300],
    'learning_rate': [0.01, 0.1],
    'max_features': ['auto', 'sqrt', 'log2']
}

CV_gboost = GridSearchCV(estimator=gboost_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_gboost.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_gboost.best_params_)

gboost_model = GradientBoostingClassifier(
    random_state=123,
    learning_rate=0.1,
    max_depth=8,
    max_features='sqrt',
    n_estimators=300
)
gboost_model.fit(train_pca_df, new_y_train)
joblib.dump(gboost_model, "model/gboost_model.joblib")

"""# **Evaluation**"""

def evaluating(y_pred, y_true):
    '''Evaluasi model'''
    labels=['Dropout', 'Enrolled', 'Graduate']

    print(classification_report(y_pred=y_pred, y_true=y_true))

    cnf_matrix = confusion_matrix(y_pred=y_pred, y_true=y_true, labels=labels)
    confusion_matrix_df = pd.DataFrame(cnf_matrix, labels, labels)
    sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
    plt.ylabel('True label', fontsize=15)
    plt.xlabel('Predicted label', fontsize=15)
    plt.show()

    return confusion_matrix_df

y_pred_test = tree_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

evaluating(y_pred=y_pred_test, y_true=y_test)

y_pred_test = rdf_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

evaluating(y_pred=y_pred_test, y_true=y_test)

y_pred_test = gboost_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

evaluating(y_pred=y_pred_test, y_true=y_test)

"""Berdasarkan ketiga model yang telah dicoba, `Gradient Boosting` memiliki nilai akurasi tertinggi dengan presentase 72%


"""

def plot_feature_importances(feature_importances, cols):
    features = pd.DataFrame(feature_importances, columns=['coef_value']).set_index(cols)
    features = features.sort_values(by='coef_value', ascending=False)
    top_features = features

    plt.figure(figsize=(10, 6))
    sns.barplot(x='coef_value', y=features.index, data=features)
    plt.show()
    return top_features

plot_feature_importances(gboost_model.feature_importances_, train_pca_df.columns)

"""# **Mengirim dataset ke dalam database**"""

URL = "postgresql://postgres.mwpouujjiofukmiqsnon:Napoleon007@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres"

engine = create_engine(URL)
jaya_df.to_sql('jaya', engine)

"""# **requirements**"""

!pip freeze > requirements.txt